\documentclass[a4paper, twocolumn]{article}
\setlength{\oddsidemargin}{0cm}
\setlength{\evensidemargin}{0cm}
\setlength{\topmargin}{0cm}
\usepackage{amsmath}
\usepackage{indentfirst}
\usepackage[utf8]{inputenc}
\usepackage{amssymb}

\begin{document}

\section{Statistische Maßzahlen}

    empirische Kovarianz
    \begin{displaymath}
        \tilde{s}_{xy} = \frac{1}{n-1} \sum^n_{i=1} (x_{i}-\bar{x}) (y_{i}-\bar{y})
    \end{displaymath} \\
    
    empirische Varianz:
    \begin{gather*}
        \sigma^2 = \tilde{s}^2 = \tilde{s}^2_x := \\
        \frac{1}{n-1} \sum^{n}_{i=1} (x_{i} - \bar{x})^2 = \frac{1}{n-1}(\sum^n_{i=1} x^2_i -n \bar x^2)
    \end{gather*}
    
    Standardabweichung
    \begin{displaymath}
      \sigma = \tilde s = \tilde s_x = \sqrt{\tilde s ^2}
    \end{displaymath} \\
    
    empirischer Korrelationskoeffizient (nach Bravais Pearson)
    \begin{displaymath}
     r_{xy} = \frac{\tilde s_{xy}}{\tilde s_x \cdot \tilde s_y}
    \end{displaymath} \\
    
    
    Spearman'sche Rangkorrelationskoeffizient von X und Y
    \begin{gather*}
      r_{SP}(x,y) = \frac
      {\sum\limits^n_{i=1} \Big(rg(x_i) - \overline{rg_x}) (rg(y_i)- \overline{rg_y} \Big) }
      { \sqrt{ \sum\limits^n_{i=1} (rg(x_i) - \bar{rg_x})^2 \cdot \sum\limits^n_{i=1} (rg(y_i) - \bar{rg_y})^2 }}
    \end{gather*}
    
    \subsection{Lineare Regression}
        optimale Ausgleichgerade:
        \begin{displaymath}
            y = f(x) = \hat a x + \hat b
        \end{displaymath}
        
        \begin{displaymath}
            \hat a = \frac{\tilde s_{xy}}{\tilde s^2_x}
        \end{displaymath}
        
        \begin{displaymath}
            \hat b = \bar y - \hat a \bar x
        \end{displaymath}
        
        Bestimmtheitsmaß, Modellgüte:
        \begin{displaymath}
          R^2 = r^2_{xy}
        \end{displaymath}
        
\section{Wahrscheinlichkeit}
	\subsection{Allgemein gilt} 
	$ P(A \cap B) = P(A) + P(B) - P(A \cup B)  $ \\
	$ P(A \cup B) = P(A) + P(B) - P(A \cap B) $ \\
	$ P(A \cap \overline B) = P(A) - P(A \cap B) $ \\
	$ P(\overline A  \cap \overline B) = P(\overline{A \cup B}) = 1 - P(A \cup B) $
	
	\subsection{für die bedingte Wahrscheinlichkeit gilt}
	\begin{gather*}
	P(B|A) = \frac{ P(A \cap B) }{ P(A) } \\
	\Leftrightarrow P(A \cap B) = P(B|A) \cdot P(A)
	\end{gather*}
	
	\subsection{totale Wahrscheinlichkeit}
	Setzt sich das Ereignis A zusammen aus den sich gegenseitig ausschließenden 
	Ereignissen $A \cap B1$ und $A \cap B2$, so gilt: \\
	$  P(A) = P(A | B_1) \cdot P(B_1) + P(A | B_2) \cdot P(B_2) $
	
\section{Zufallsvariablen}
	\subsection{Dichtefuntion} $ \mathbb{P}(X=x) = f(x) $ ist einfach die Wahrscheinlichkeit für x.

	\subsection{Verteilungsfunktion} $ \mathbb{P}(X \le x) = F(x) $ gibt die Wahrscheinlichkeit an, dass X kleiner oder gleich x ist.
	
	\subsection{Erwartungswert bei diskreter ZV}
	Dieser Wert ist als Mittel zu erwarten.
	\begin{gather*}
		\mathbb{E}(X) = \sum_i x_i \cdot f(x_i)
	\end{gather*}




\end{document}
    